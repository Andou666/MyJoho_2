{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 情報とデータサイエンス"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習内容について\n",
    "　この章での学習は，学習指導要領解説にある（ア）（イ）（ウ）を統合したものになっている。これは，データを分析する過程が，（ア）（イ）（ウ）の一連の流れを必要とするからである。そのため，学習11と学習12に関しては，（ア）の内容を中心として書いてあるが，それ以外の学習に関しては，全ての要素を含み，データ分析手法の違いに着目して書いている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## この章の学習について\n",
    "　この章の学習については，Webスクレイピング，データクリーニングをはじめとして，機械学習やニューラルネットワークの基礎的な事項が学べるように配慮している。  \n",
    "　「情報Ⅰ」でも学んだように，WebコンテンツやWebに掲載されたデータを取り込むことをWebスクレイピングといい，そのデータを分析，処理するソフトウェアに読み込ませ，分析しやすいようにするために整理，整形することをデータクリーニングという。これは，主に学習の11と12で扱う。  \n",
    "　学習13から18までの学習では主に機械学習といわれている分野の内容を扱い，一連のデータ分析の流れを体験できるような内容としている。機械学習は，教師あり学習（supervised learning），教師なし学習（unsupervised learning），強化学習（reinforcement learning）の三つに大別できるが，本教材では教師あり学習と教師なし学習についてのみ扱う。学習の内容は各学習の中で解説するが，教師あり学習とは，データに対してラベルが付いているものに関して機械学習を行う手法であり，写真データに「犬」や「猫」などの正解ラベルを含んでいるものである。一方で教師なし学習とは，正解ラベルが付いていないデータに関する学習であり，それぞれ目的によって，使い分ける必要がある。どのような目的で分析するかによって，選ぶ手法や学習の種類は異なるが，それに関しての詳細は各学習の中で解説する。  \n",
    "　データを分析するために必要なスキルを効率よく学ぶには，学習11と12のデータの収集や整理，整形について学び，それぞれのデータ分析の目的や手法に合わせて，学習13から18までを任意に選択するとよいだろう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 本章の学習での演習環境\n",
    "　本章の演習においては，表計算ソフトウェア，統計処理ソフトウェアR，プログラミング言語Pythonを想定している。Rに関しては，v.3.6以上，統合開発環境であるRStudioの利用が望ましい。Pythonに関しては，v.3.7以上で，AnacondaのJupyter Notebook，Jupyter Labo，Spyderなどの統合開発環境での利用を想定している。また一部の学習では，Google Colaboratoryも活用している。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データと関係データベース"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの信憑性と信頼性\n",
    "1. データの信憑性\n",
    "\n",
    "　データとはデータ分析に使うもとになるものをいい，データ分析とはそのデータの傾向や性質を数量で捉えることをいう。データは様々な方法で収集することができ，集まったデータの形式も様々である。したがって，信憑性のあるデータの収集や分析をするためには注意すべき点が多くある。  \n",
    "　この学習項目では，データ処理の方法によって同じデータでも見え方が異なることを体験するとともに，データの収集や分析の際に起こりうる基本的なデータの偏りについて確認する。  \n",
    "　データの信憑性（credibility）とは，示されたデータが信じるに値する科学的根拠となり得るかである。Cambridge Dictionary によると credible は「able to be trusted or believed」とされている。演習 1 では，同じデータであっても目盛りの取り方や階級の幅（ビン幅）の取り方，外れ値の扱いにより異なったヒストグラムとなることを確認し，データの信憑性をデータの提供者の視点から考える 図表１ 。\n",
    " \n",
    "　ヒストグラムでは適切な階級の幅を設定することが重要である。分析のために同じデータでいくつかの階級の幅を試すことも有用である。適切な階級の数は計算で求める方法もいくつかある。例としてスタージェスの公式を挙げておく。\n",
    " \n",
    "$$ (階級の数) = 1+log_2n (nはデータの個数) $$ \n",
    " \n",
    "　また，何らかの原因により，収集したデータの中で他と大きく異なる値を外れ値という。外れ値からデータ全体の重大な欠陥や特徴が見えることもあるため，外れ値には注意を払う必要がある。\n",
    "\n",
    "\n",
    "2. 母集団と調査\n",
    "\n",
    "　データの収集，読み解き（理解）の際には母集団に気を付けて標本の大きさ（サンプルサイズ）や標本抽出（サンプリング）の方法を選択する必要がある。母集団を適切に把握しないと集めたデータに偏り（バイアス）が生じることもあるため，母集団を意識してデータ収集を行い，提示されたデータを読み解く必要がある。  \n",
    "母集団全体を調査することを全数調査，悉皆調査という。日本では 5 年に一度行われる国勢調査が代表例である。母集団全体を調査できないときには有意抽出や無作為抽出を用いる。有意抽出とは母集団の様々なカテゴリから適切な割合を見ながら作為的に標本をとることである。無作為抽出では特定の属性を持ったデータに偏らないようにランダムに選んだ標本を使う。主にアンケート調査や視聴率調査にも使われている。サンプルサイズが小さすぎると信頼性は低くなる。適切なサンプルサイズは「数学 B」で学習する計算式を用いて求めることができる。国の調査などでは一般に\n",
    "信頼水準 95％を用いている。\n",
    "\n",
    "$$ n = 1.96・\\frac{p(1-p)}{d^2} $$\n",
    "\n",
    "(nはサンプルサイズ,pは回答率,dは許容誤差) * 信頼水準は99\\%では1.96の代わりに2.58を使う\n",
    "\n",
    "\n",
    "　これを簡単に計算できるフォームを公開しているWeb サイトもある。アンケートを実施するなどデータ収集の際には必要なサンプルサイズに気を付けたい。\n",
    " \n",
    " \n",
    "3. バイアス\n",
    "\n",
    "　バイアスには選択バイアス，生存バイアスなどがある。選択バイアスには，標本抽出の際に発生する標本バイアス（母集団の取り違えなど意識的，無意識的によらず無作為でない抽出）や膨大探索効果（膨大なデータに対して様々なモデルを当てはめると何かしらの興味深い結果が出るが，それは本当に興味深い結果か偶然のいずれかである），特定の分析結果を強調するための時間間隔をとること，分析者にとって都合の良い結果が見えたところで検定を止めることなどが含まれる。生存バイアスは，以下の例に示すように母集団全体ではなく偏ったデータ抽出をしてしまうことで\n",
    "ある。  \n",
    "　例えば，お菓子の売れ行きが芳しくないときにそのお菓子の購入者にアンケートを採ることや，ランダムで発生させた番号の固定電話に電話をかけて聞き込み調査をするといったことである。お菓子の購入者にアンケートを採っても肝心のお菓子を買わなかった人は含まれていないし，固定電話の電話番号にかけても固定電話を持たない人は含まれていないので調査結果に偏りがあることは容易に想像できるはずだが，意外と見落としがちなバイアスである。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの形式と蓄積方法\n",
    "\n",
    "　この学習項目では，表形式によるデータの保持とデータの型，関係データベースによるデータの蓄積方法について学ぶ。  \n",
    "　ビッグデータという言葉が一般的になるくらい身の回りにデータがあふれている。しかし，それらのデータは紙にメモ書きされたようなものから，種類ごとにまとめられたもの，観測値のように数値だけが集積されたものまである。そしてそれらのデータは往々にしてそのままでは使いづらい。例えば，地震の観測値（常にどのくらい揺れているかを測定している値）は数値の羅列であり，事前に何を表す値なのか分かっている人に説明してもらわなければ，見る人にその数値の意\n",
    "味が分からない。一方で，人にとってデータの意味を分かりやすいようにすると，データサイエンスでデータ処理する際には扱いづらい。例えば，通知表のように各生徒の履修科目とその成績を印刷すると，一人一人の生徒にとっては分かりやすいが，コンピュータではそれらのデータは処理しづらい。  \n",
    "　ここで，人にもある程度データの意味を読むことができ，コンピュータでも処理しやすい形でデータを保持，蓄積する方法として表形式データがある。表形式データを蓄積する方法として関係データベース（リレーショナルデータベース，以後RDBと表記）がある。他にもデータの保持，蓄積方法はあるが，それらについてはこの学習の最後の節で扱う。  \n",
    "　表形式データは，表計算ソフトでの扱いのように行（ロウ）と列（カラム）からなるデータ保持である 図表３ 。この表形式のデータをRDBではテーブルと呼ぶ場所を用意して格納している。テーブルでは列名を表示するためにヘッダーを用意し，データベースの構造であるスキーマや各列の値の種類を示す型（一例を 図表２に挙げている）が分かりやすいように指定する。RDBでは行のことをレコード（タプル），列のことを属性（カラム）という。RDBを操作するためにはSQL（Structured Query Language）を使う。SQLにはMySQLやSQLiteなどの無料のものから有料のものまで様々ある。また，データを処理するハードウェアが1台で完結することを想定していることが多く，複数のハードウェアで大量のデータを分散処理することは得意ではない。表形式で整理されていない大量のデータを高速に分散処理することは，本学習の最後の項「4．データベースの種類とその使い分け」で述べるNoSQLの方が得意である。NoSQLにも様々な種類があり，目的に応じて使い分けることが求められている。\n",
    " \n",
    "|データ型|例|用途|\n",
    "|:---|:---|:---|\n",
    "|Char（固定長）<br>（varchar可変長）| Abc | 文字，文字列を表す|\n",
    "|String（text）| Abc | 文字列を表す|\n",
    "|Int | 1, 2, 3 | 整数を表す|\n",
    "|Real（Double, Float）| -0.2, 3.4 | 実数（または浮動小数点数）を表す|\n",
    "|Object（blob）| 123abc | 図，音声などのデータ| \n",
    "\n",
    "　RDBは表計算ソフトや2次元配列（行列）のような形をしている 図表３ 。ヘッダーを付けない場合もあるが，付ける際にはスキーマや格納するデータの型が分かりやすいように付ける。レコード（タプル）は表の行を表し，一つ一つのデータである。属性（カラム）は列を指し，データセットの特徴量（特定の種類）を表す。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データサイエンスでのデータの利用\n",
    "\n",
    "　この学習項目では，簡単なデータ操作ではなく，大量のデータをプログラミングで処理，分析するデータサイエンスでのデータ利用の基本操作や用語を確認する。  \n",
    "　プログラミング言語では，データベースやデータをそのまま扱うこともできるが，大変面倒である。そのためRやPythonではデータフレームと呼ばれる形でデータを扱う 図表４ 。データを扱うことに向いたこれらの言語ではRDBをSQLで直接操作するよりもずっと簡単に高速にデータを読み込み，適切な形で処理することができる。  \n",
    "　実際にGoogle DriveとGoogle Colaboratory（ 以下Colabと略す）を使ってPythonでデータフレームを作成し，データの読み込みとデータ操作をしてみよう。まず，データフレームに読み込むデータTestdata.csv（別添資料）をColabからアクセスできる場所に置くか，Google Driveにあるファイルならば，Google Driveをマウントする※。Google Driveを別ウィンドウで開いてマイドライブの中にできているColab Notebooks（Colabのためのフォルダ）にTestdata.csvを置く。これはあるクラス39人の小テストのダミーデータである。以下のたった数行のコードで表形式のデータを読み込み，データフレームとして扱うことができる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Testdata.csv')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "　１行目はPythonでデータフレームを扱うためのパッケージの一つであるpandasを使えるようにしている。pdと名付けている。2行目のread_csv関数でデータフレームをdfとして作成している。（）内の「 '（引用符）」ではさまれた部分が読み込みたいデータのパス（最初にTestdata.csvを置いた場所）である。3行目はprint文を使って作成したデータフレームdfの中身を出力する。確認が不要であれば書く必要はない。\n",
    "　pandasのデータフレームでは，axis=0で同じ列内のデータ，axis=1で同じ行内のデータを対象に，集約計算やデータ削除の処理を行う。また，インデックスを使って行を指定することができる。これにより簡単に大量のデータから列や行の指定，削除ができる。重複データや欠損値データの抽出機能もある。欠損値の削除や補完は学習12で扱う。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データベースの種類とその使い分け\n",
    "　この学習項目ではデータベースの種類を知り，それぞれのデータ蓄積方法の利点と欠点を考える。データベースには大きく分けてSQL構文を使い操作を行うRDBと，RDBやRDBMS以外のNoSQLとが\n",
    "ある。現在ではNoSQLはNot only SQLの略とされることが一般的であり，RDBとRDB以外のデータ\n",
    "ベース両方を使う手法が登場している。  \n",
    "　大量のデータを扱う際にRDBは必ずしも効率的な方法ではなく，NoSQLでは，これらのデータをキー・バリュー型，カラム型，ドキュメント型，グラフ型などの形式で蓄積している。\n",
    "解答資料に，より詳しくNoSQLを学びたい方に向けて，いくつかの無料枠や学習素材を提供しているデータベースを紹介している。\n",
    "\n",
    "\n",
    "「2.データの形式と蓄積方法」（112ページ）では，データ保持の形式として表形式を扱った。ここでは，JSON（JavaScript Object Notation）形式について基本的なことを確認しておく。JSON形式は，NoSQLでもドキュメント型で代表されるように，表形式では保持しにくいデータに対応した形式であり，国が公開しているデータベースでも提供されている形式である。  \n",
    "　図表６ のように｛　と　｝で囲まれた部分でひとまとまりのデータを表し，「：」で区切った左側をキー（名前），右側をバリュー（値）という。「，」で区切って複数のデータを同じ｛　｝内に並べることもできる。また，入れ子構造となることや配列（［と］で囲まれた部分）を要素とすることも多い。 図表６ では人間が理解しやすいようにインデントや改行が入っているが，これはあってもなくてもよい。コンピュータが処理する際にはこのインデントや改行はなく，全て続いた状態で保持され，読み込みも行われる。  \n",
    "　113ページのCSVファイルの場合と同様に，Pythonではたった数行で簡単にJSON形式のデータをプ\n",
    "ログラムに読み込み，使うことができる。学習23以降では，同様にJSON形式のデータを読み込み，更に活用している。\n",
    "\n",
    "```\n",
    "{\n",
    " \" 名前 \":\" たろう \",\n",
    " \" 年齢 \":17,\n",
    " \" 履修科目 \":{\n",
    " \" 修得済 \":{\n",
    " \" 国語 \":[\" 国語総合 \"],\n",
    " \" 数学 \":[\" 数学１\",\" 数学 A\"],\n",
    " \" 情報 \":[\" 情報１\"]\n",
    " },\n",
    " \" 未修得 \":{\n",
    " \" 情報 \":[\" 情報２\"]\n",
    " }\n",
    " }\n",
    "}\n",
    "```\n",
    "\n",
    "　更に余裕があれば自分で書いたJSON形式のデータが正しい文法で書けているかを検証できる無料のサイトがあるので，ぜひ試してみてほしい。他にもJSON形式で表現したデータをアップロードして他の人と共有できるサイト，他の人が用意したオープンソースのデータを利用できるサイトがある。アイスクリームの講評データを集めて公表したものや，急患診療事業情報といった面白そうなデータから役立ちそうなデータまで様々あるので，ぜひ探して分析や開発に利用してほしい（参考文献参照）。  \n",
    "　また，NoSQLで使用されるデータ保持の形式としてLOD（Linked Open Data）もある。発展的な学習の参考までに，解答資料で触れているので様々なデータ保持の形式を見てほしい。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【参考文献・参考サイト】\n",
    "- 「なるほど統計学園高等部」総務省統計局 https://www.stat.go.jp/koukou/index.html\n",
    "- 「政府統計の総合窓口（e-Stat）」総務省統計局 https://www.e-stat.go.jp/\n",
    "- 「データサイエンスのための統計学入門」Peter Bruce，Andrew Bruce 著　黒川利明 訳　大橋真也 技術監修　オライリージャパン（2018）\n",
    "- 「東京大学のデータサイエンティスト育成講座」塚本邦尊，山田典一，大澤文孝 著　中山浩太郎 監修　松尾豊 協力　マイナビ出版（2019）\n",
    "- 「[ 第 2 版 ] Python 機械学習プログラミング」Sebastian Raschka，Vahid Mirjalili 著　株式会社クイープ 訳　福島真太朗 監訳　インプレス（2018）\n",
    "- 「NOSQL の基礎知識」太田洋 監修　本橋信也，河野達也，鶴見利章 著　リックテレコム（2012）\n",
    "- 「navischola.app Schola Scope」https://navischola.app/network/6/general-science-and-engineering/\n",
    "- 「JSON.org」https://www.json.org/json-en.html\n",
    "- 「LinkData.org」http://linkdata.org/work?sort=date\n",
    "- 「Cambridge Dictionary」CambridgeUniversity Press 2020 https://dictionary.cambridge.org/ja/dictionary/\n",
    "- 「The Linked Open Data Cloud」https://lod-cloud.net/\n",
    "- 「DBpedia Jpanese」http://ja.dbpedia.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 大量のデータの収集と整理・整形"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの収集\n",
    "　データを収集する際には，アンケートのような調査からの取得，計測機器からの取得，Web APIからの取得などの方法がある。国内のWebサイトだけでなく，海外のWebサイトにも目を向けると多様なデータを取得することができる。ここでは，USGS（U.S.Geological Survey，アメリカ地質調査所）のWebサイトで提供されているWeb APIを用いて，データを取得してみよう。\n",
    " \n",
    "　一般にWeb APIを用いてデータを取得する際は，これを提供するサイトのユーザー登録とAPIを利用するためのキー（特定の文字列）の取得が必要である。ここでは，登録なしで利用できるUSGSのEarthquake CatalogのAPIを使って実習してみよう。  \n",
    "　ブラウザのアドレスバーに，APIのエンドポイントのURL（ https://earthquake.usgs.gov/fdsnws/event/1/[METHOD[?PARAMETERS]] ） のMETHODにはメソッド名，PARAMETERSには必要なパラメータを与えてアクセスすることによりデータを取得できる。例えば，URLを「 https://earthquake.usgs.gov/fdsnws/event/1/query?format=geojson&starttime=2019-12-01&endtime=2019-12-02 」としてアクセスすることにより，2019年12月1日から2日に発生した地震のデータをGeoJSON形式で取得できる 図表１ 。詳細な説明はAPI Documentationのページを参照されるとよい。\n",
    " \n",
    "\n",
    "　他にも，Webサイトにはオープンデータを含め，様々なデータが掲載されており，十分な価値がある。また，公式サイトの説明文や口コミサイトなどのコメントといったテキストデータも収集し分析することで，有用な情報が得られる。自動でWebサイトを巡回してデータをかき集めることをクローリングといい，そのデータを解析して必要なデータを抽出することをスクレイピングという。これらを合わせてWebスクレイピングといい，Pythonは，requestとBeautiful Soup4というパッケージ，Rはrvestというライブラリを用いる。\n",
    " \n",
    "　文部科学省の新着情報のページのHTMLでは日付に，class属性がinformation-dateであるh3タグが付いている。項目は1日分ごとにclass属性がnews_listであるulタグ内に記述され，1記事ごとにaタグが付けられている。これらの特徴を基に掲載日ごとに項目名を取得するPythonによるプログラムは，次の通りである。  \n",
    "　Webスクレイピングを用いることにより，Webサイトに掲載された様々なデータの取得が可能となるが，繰り返し文などにより多数回アクセスを試みたり，画像などのファイルサイズの大きいファイルを多数取得したりすることは，対象となるWebサーバに必要以上の負荷をかけることから，プログラムの実行の可否を検討する必要がある。また，Webスクレイピングを利用規約で禁止しているWebサイトもあるので，利用規約を確認することも必要である。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "次世代計算基盤検討部会（第5回）開催案内\n",
      "【総合職事務系】業務説明会の日程一覧【NEW】\n",
      "諸外国の高等教育\n",
      "令和3年度 「知識集約型社会を支える人材育成事業」メニューⅢ「インテンシブ教育プログラム」公募説明会の開催について\n",
      "令和3年度「知識集約型社会を支える人材育成事業」メニューⅢ「インテンシブ教育プログラム」公募について\n",
      "令和3年度大学教育再生戦略推進費「大学の世界展開力強化事業」 ～アジア高等教育共同体（仮称）形成促進（国際質保証制度設計業務）～公募について\n",
      "萩生田光一文部科学大臣記者会見録（令和3年4月13日）\n",
      "新しい時代の学校施設検討部会（第2回）の開催について\n",
      "再生・細胞医療・遺伝子治療研究の在り方に係る検討会（第2回）の開催について\n",
      "デジタル教科書の今後の在り方等に関する検討会議（第10回）の開催について\n",
      "文部科学省大臣官房会計課非常勤職員（時間雇用職員）採用のお知らせ\n",
      "文部科学省大臣官房会計課非常勤職員（期間業務職員）採用のお知らせ\n",
      "研究計画・評価分科会（第76回）の開催について\n",
      "映像作品等選定一覧（令和3年3月）\n",
      "第4期中期目標期間における国立大学法人運営費交付金の在り方に関する検討会（第7回）の開催について\n",
      "萩生田光一文部科学大臣記者会見録（令和3年4月9日）\n",
      "高等教育の修学支援新制度の対象機関リスト（全機関要件確認者の公表情報とりまとめ）\n",
      "科学技術・学術政策研究所「科学技術の状況に係る総合的意識調査（NISTEP定点調査2020）」報告書の公表について\n",
      "令和２年度実施報告書（平成29年～令和２年度）\n",
      "令和元年度公立学校教職員の人事行政状況調査について\n",
      "令和2年度「障害者の生涯学習支援活動」に係る文部科学大臣表彰について\n",
      "端末利用に当たっての児童生徒の健康への配慮等に関する啓発リーフレットについて\n",
      "児童生徒の自殺予防に関する調査研究協力者会議（令和2年度）（第3回）　配付資料\n",
      "「やってみよう！登下校見守り活動ハンドブック」の公表について\n",
      "令和3年度「「富岳」成果創出加速プログラム」（高性能汎用計算機高度利用事業費補助金）の公募について\n",
      "革新的将来宇宙輸送システム実現に向けたロードマップ検討会（第8回）の開催について\n",
      "令和3年度大学教育再生戦略推進費「大学の世界展開力強化事業」～アジア高等教育共同体（仮称）形成促進～ 公募要領（案）の掲載について\n",
      "文部科学省大臣官房総務課広報室非常勤職員採用（期間業務職員）のお知らせ\n",
      "令和3年度「学力調査を活用した専門的な課題分析に関する調査研究」（全国学力・学習状況調査のCBT化に向けた令和3年度試行・検証）の事前説明会の開催について\n",
      "産学連携による医薬品・医療機器等の研究開発の推進の在り方に関する検討会（第2回）の開催について\n",
      "不登校児童生徒等又は療養等による長期欠席生徒等を対象とする特別の教育課程を編成して教育を実施する学校に関する指定要項等 \n",
      "萩生田光一文部科学大臣記者会見録（令和3年4月6日）\n",
      "コミュニティ・スクールの在り方等に関する検討会議の設置について\n",
      "令和3年度科学技術分野の文部科学大臣表彰受賞者の決定について\n",
      "文部科学省における新型コロナウイルス感染者の発生について\n",
      "公立義務教育諸学校の学級編制及び教職員定数の標準に関する法律の一部を改正する法律\n",
      "萩生田光一文部科学大臣記者会見録（令和3年4月2日）\n",
      "生命倫理・安全部会（第46回）の開催について\n",
      "令和３年度統計エキスパート人材育成プロジェクトの公募について\n",
      "令和２年度教科用図書検定調査審議会総会（第４回）　配付資料\n",
      "「世界自閉症啓発デー」を迎えるに当たっての文部科学大臣メッセージについて（令和3年4月2日）\n",
      "情報委員会（第16回）の開催について 【オンライン会議】\n",
      "「教育現場におけるオンライン教育の活用」に関する取りまとめについて\n",
      "令和3年度「特色ある共同研究拠点の整備の推進事業～機能強化支援（一般型）～」採択拠点一覧\n",
      "令和3年度「特色ある共同研究拠点の整備の推進事業～スタートアップ支援～」採択拠点一覧\n",
      "令和3年度からの共同利用・共同研究拠点の認定について（公立大学、私立大学）\n",
      "令和3年度研究開発学校の指定について\n",
      "文部科学省所管独立行政法人の理事長・館長の任命について\n",
      "「地域社会に根ざした高等学校の学校間連携・協働ネットワーク構築事業（COREハイスクール・ネットワーク構想）」の採択結果について\n",
      "災害時における学校給食実施体制の構築に関する事例集（令和3年3月）\n",
      "文部科学広報2021年3月号\n",
      "第10期ナノテクノロジー・材料科学技術委員会（第9回）議事録\n",
      "令和2年度文部科学省研究開発評価シンポジウム\n",
      "令和元年度高等学校及び中等教育学校における「通級による指導」実施状況調査の実施について（結果）\n",
      "今後の主権者教育の推進に向けて（最終報告）\n",
      "「第5次国立大学法人等施設整備5か年計画」の公表について\n",
      "文部科学省公式サイトトップページをリニューアルしました\n",
      "萩生田光一文部科学大臣記者会見録（令和3年3月30日）\n",
      "（2021年3月） 令和3年度に行われる教科用図書検定結果の公開について\n",
      "「外国人留学生在籍状況調査」及び「日本人の海外留学者数」等について\n",
      "政策評価の結果の政策への反映状況（令和２年度）\n",
      "「令和の日本型学校教育」の構築を目指して～全ての子供たちの可能性を引き出す，個別最適な学びと，協働的な学びの実現～（答申）（中教審第228号） 【令和3年3月30日更新】\n",
      "大学入試のあり方に関する検討会議（第24回）の開催について\n",
      "令和3年度「国際原子力人材育成イニシアティブ事業」の公募について\n",
      "基礎研究医養成活性化プログラム（令和3年度選定）\n",
      "令和３年度学校基本調査について\n",
      "「海洋生物ビッグデータ活用技術高度化」の公募について\n",
      "令和2年度科学技術イノベーションによる地域社会課題解決（DESIGN-i）評価結果について\n",
      "大学等設置に係る寄附行為（変更）認可後の財政状況及び施設等整備状況調査結果について（令和2年度）\n",
      "設置計画履行状況等調査の結果について（令和2年度）\n",
      "萩生田光一文部科学大臣記者会見録（令和3年3月26日）\n",
      "「#教師のバトン」プロジェクトについて\n",
      "学校教育法第三十四条第二項に規定する教材の使用について定める件の一部を改正する件の公布及び施行等について（通知）\n",
      "専修学校の質の保証・向上に関する調査研究協力者会議（第20回）　議事録\n",
      "文部科学省研究振興局非常勤職員（期間業務職員）採用のお知らせ\n",
      "「国立大学法人ガバナンス・コード」に関する協力者会議の開催について\n",
      "学校教員統計調査-令和元年度（確定値）結果の概要-\n",
      "文部科学省大臣官房総務課非常勤職員（期間業務職員）採用のお知らせ\n",
      "政策評価に関する有識者会議（第54回）　配付資料\n",
      "大学入学者選抜における多面的な評価の在り方に関する協力者会議（第12回）の開催について\n",
      "専門学校（専修学校専門課程）における「キャリア形成促進プログラム」認定（令和2年度）について\n",
      "主権者教育推進会議（第19回）の開催について\n",
      "令和2年度「学術情報基盤実態調査」の結果報告について－大学における大学図書館及びコンピュータ・ネットワーク環境の現状について－\n",
      "文部科学省大臣官房総務課広報室非常勤職員採用（時間雇用）のお知らせ\n",
      "独立行政法人の公募結果について\n",
      "全国的な学力調査のCBT化検討ワーキンググループ（第8回）の開催について\n",
      "令和3年度スーパーサイエンスハイスクール（SSH）指定校の内定等について\n",
      "「人を対象とする生命科学・医学系研究に関する倫理指針」の制定について\n",
      "特定先端大型研究施設の共用の促進に関する法律第２８条第２項に基づく公示について（特定中性子線施設）\n",
      "特定先端大型研究施設の共用の促進に関する法律第２８条第２項に基づく公示について（特定放射光施設）\n",
      "「一家に1枚 海 ～その多様な世界～」及び科学技術週間周知ポスターのウェブサイトへの掲載について\n",
      "教職経験者研修・職階研修その他の研修等実施状況（令和元年度）調査結果について\n",
      "萩生田光一文部科学大臣記者会見録（令和3年3月23日）\n",
      "中堅教諭等資質向上研修実施状況（令和元年度）調査結果について\n",
      "初任者研修実施状況（令和元年度）調査結果について\n",
      "令和2年度教科用図書検定調査審議会総会（第４回）の開催について\n",
      "科学技術･学術審議会学術分科会研究費部会（第11期第1回）の開催について\n",
      "「全国学生調査」に関する有識者会議（第4回）議事録\n",
      "がん研究の推進の在り方に関する検討会（第3回）の開催について\n",
      "「情報科学を活用した地震調査研究プロジェクト（STAR-Eプロジェクト）」の公募について\n",
      "カーボン・ニュートラル達成に向けた大学等の貢献に係る学長等サミット参加者等について\n",
      "大学分科会（第159回）　議事録\n",
      "児童生徒の自殺予防に関する調査研究協力者会議（令和2年度第3回）の開催について\n",
      "法科大学院公的支援見直し強化・加算プログラムの審査結果について\n",
      "文部科学省における新型コロナウイルス感染者の発生について\n",
      "専修学校の質の保証・向上に関する調査研究協力者会議（第21回）の開催について\n",
      "革新的将来宇宙輸送システム実現に向けたロードマップ検討会（第7回）の開催について\n",
      "令和2年度大学等卒業予定者の就職内定状況調査（2月1日現在）\n",
      "カーボン・ニュートラル達成に向けた大学等の貢献に係る学長等サミットについて\n",
      "第11期科学技術・学術審議会国際戦略委員会（第1回）の開催について\n",
      "文部科学省　教育データ標準\n",
      "【第3回】消費者教育推進委員会開催案内\n",
      "文部科学省における新型コロナウイルス感染者の発生について\n",
      "令和3年度「先端研究基盤共用促進事業（コアファシリティ構築支援プログラム）」の公募について\n",
      "令和3年度「先端研究基盤共用促進事業（先端研究設備プラットフォームプログラム）」の公募について\n",
      "第46回人間と生物圏(MAB)計画分科会の開催について\n",
      "「官民協働海外留学支援制度～トビタテ！留学JAPAN日本代表プログラム～」第14期応募状況について\n",
      "「大学の世界展開力強化事業」（平成30年度選定）の中間評価結果について\n",
      "「大学の世界展開力強化事業」（平成27年度選定）の事後評価結果について\n",
      "GIGAスクール構想の実現に向けた調達等に関する状況について\n",
      "令和2年度優れた「早寝早起き朝ごはん」運動の推進にかかる文部科学大臣表彰について\n",
      "科学技術・学術審議会（第65回）配布資料\n",
      "デジタル教科書の今後の在り方等に関する検討会議　中間まとめ\n",
      "令和3年度「学校教育における外部人材活用事業」の公募について\n",
      "文部科学省大臣官房人事課【障害者雇用（期間業務職員・時間雇用職員）】採用のお知らせ \n",
      "文部科学省における新型コロナウイルス感染者の発生について\n",
      "萩生田光一文部科学大臣記者会見録（令和3年3月16日）\n",
      "令和3年度「英知を結集した原子力科学技術・人材育成推進事業」における新規採択課題の公募開始に関するお知らせ ～課題解決型廃炉研究プログラム～\n",
      "産学連携による医薬品・医療機器等の研究開発の推進の在り方に関する検討会（第1回）　配付資料\n",
      "文部科学省研究開発局海洋地球課非常勤職員（期間業務職員）採用のお知らせ\n",
      "令和二年度技術士第二次試験の実施について（追試験）\n",
      "「感染症医療人材養成事業」の選定結果について\n",
      "国立研究開発法人審議会 量子科学技術研究開発機構部会（第20回）の開催について\n",
      "全国的な学力調査に関する専門家会議（平成31年4月12日～）（第8回）の開催について\n",
      "第5回インフラメンテナンス大賞の募集を開始します！\n",
      "国立研究開発法人審議会 宇宙航空研究開発機構部会（第20回）の決定事項について\n",
      "令和２年度学校法人の運営等に関する協議会\n",
      "「高校生のための学びの基礎診断」に関する有識者会議（第4回）の開催について\n",
      "大学改革推進等補助金「大学保有検査機器活用促進事業」の公募について\n",
      "総合職技術系説明会情報\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "url = 'https://www.mext.go.jp/b_menu/news/index.html'\n",
    "r = requests.get( url )\n",
    "soup = BeautifulSoup( r.content, 'html.parser' )\n",
    "links = soup.find_all( 'ul', 'news_list' )\n",
    "for l in links:\n",
    "  titles = l.find_all('a')\n",
    "  for t in titles:\n",
    "    print( t.string )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 収集したデータの整理　\n",
    "　収集したデータはそのままではプログラムで扱うことができない場合が多い。例えば，表の題名が付いていたり，不要な列が付加されていたりする。これはプログラミング言語のデータの形式と合っていないことが原因である。また，データの表記にゆれがあり，同じデータが異なったデータとして扱われてしまうことも起こりうる。更に，同じデータが重複して存在することもある。そのため，表計算ソフトなどを用いて，これらを事前に整理する必要がある。表計算ソフトのフィルター機能を用いることで，列に含まれるデータの一覧を表示することができ，該当データを抽出して修正する。このような修正をデータクリーニングという。\n",
    " ダウンロードしたファイルは，不要な行や列が含まれるなどそのままプログラムで操作するには適していない。このファイルに対して，表計算ソフトを用いて次の操作を行う 図表3，4 。\n",
    " \n",
    "- 不要な行や列を削除する\n",
    "- 項目名を修正する\n",
    "- 不要なカンマが付与されないようセルの書式を「通貨」から「標準」に変更する\n",
    "- プログラムでファイルを開くことができるよう「CSV UTF-8形式」で保存する\n",
    "\n",
    "　このデータは比較的修正が少なくて済むデータであったが，他に次のような修正が考えられる。\n",
    " \n",
    "- 表記のゆれを修正する（大文字と小文字，西暦と和暦，正式名称と略称，空白の有無など）\n",
    "- 不要なデータの注釈や空白文字を除去する\n",
    "- 重複するデータを除去する\n",
    "\n",
    "公開されているデータには，印刷することを前提に整形されているものも多く，このようなデータを，プログラムを用いて処理するには，このように前処理をする必要がある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データフレームを用いたデータの操作\n",
    "　学習11ではリレーショナルデータベースについて学習し，データフレームについても簡単に扱った。Pythonではpandasというパッケージを使うことにより，データを抽出したり，集計したりというように，まとめてデータを扱うことができるようになる。他には，リレーショナルデータベースのように二つのデータを結合したり，重複データを取り除いたりすることができる。\n",
    "\n",
    "　pandasを用いてデータフレームにデータを読み込む。読み込んだデータフレームの出生数，転入者数，死亡数，転出者数から増減を計算し，新しい列として追加する。同様に人口増減率を求めて，新しい列を追加する。人口増減率が多い都道府県が分かるように，df.sort_values（'増減率', ascending=False）によりデータをソートする。ascending=Falseとすることで，降順でソートすることができる図表５ 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>都道府県</th>\n",
       "      <th>総人口</th>\n",
       "      <th>出生数</th>\n",
       "      <th>死亡数</th>\n",
       "      <th>転入者数</th>\n",
       "      <th>転出者数</th>\n",
       "      <th>増減</th>\n",
       "      <th>増減率</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>東京都</td>\n",
       "      <td>13515271</td>\n",
       "      <td>113194</td>\n",
       "      <td>111673</td>\n",
       "      <td>456635</td>\n",
       "      <td>372404</td>\n",
       "      <td>85752</td>\n",
       "      <td>0.634482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>沖縄県</td>\n",
       "      <td>1433566</td>\n",
       "      <td>16941</td>\n",
       "      <td>11326</td>\n",
       "      <td>26384</td>\n",
       "      <td>26476</td>\n",
       "      <td>5523</td>\n",
       "      <td>0.385263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>神奈川県</td>\n",
       "      <td>9126214</td>\n",
       "      <td>73475</td>\n",
       "      <td>75762</td>\n",
       "      <td>225815</td>\n",
       "      <td>208539</td>\n",
       "      <td>14989</td>\n",
       "      <td>0.164241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>愛知県</td>\n",
       "      <td>7483128</td>\n",
       "      <td>65615</td>\n",
       "      <td>64060</td>\n",
       "      <td>127036</td>\n",
       "      <td>116518</td>\n",
       "      <td>12073</td>\n",
       "      <td>0.161336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>埼玉県</td>\n",
       "      <td>7266534</td>\n",
       "      <td>56077</td>\n",
       "      <td>62565</td>\n",
       "      <td>180451</td>\n",
       "      <td>162374</td>\n",
       "      <td>11589</td>\n",
       "      <td>0.159485</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    都道府県       総人口     出生数     死亡数    転入者数    転出者数     増減       増減率\n",
       "12   東京都  13515271  113194  111673  456635  372404  85752  0.634482\n",
       "46   沖縄県   1433566   16941   11326   26384   26476   5523  0.385263\n",
       "13  神奈川県   9126214   73475   75762  225815  208539  14989  0.164241\n",
       "22   愛知県   7483128   65615   64060  127036  116518  12073  0.161336\n",
       "10   埼玉県   7266534   56077   62565  180451  162374  11589  0.159485"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# 参考\n",
    "# https://www.e-stat.go.jp/stat-search/\n",
    "# 「社会・人口統計体系 / 社会生活統計指標－都道府県の指標－2020 / 基礎データ」加工データ\n",
    "df = pd.read_csv(\"population.csv\")\n",
    "df['増減'] = (df['出生数']+df['転入者数']) - (df['死亡数']+df['転出者数'])\n",
    "df['増減率'] = df['増減'] / df['総人口'] * 100\n",
    "df.sort_values('増減率', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-283642"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['増減'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ロングフォーマットとワイドフォーマット\n",
    "　表形式のデータにはワイドフォーマット（横持ち形式）とロングフォーマット（縦持ち形式）という二つの形式がある。ワイドフォーマットは 図表６ の商品データの例のように，一つの商品の全ての属性を，横に並べた列を使い，1行で保持する形式である。ロングフォーマットは，図表７ の例のように，同じ商品であっても属性ごとに行を縦に増やして，全ての属性の情報を保持する形式である。  \n",
    "　ワイドフォーマットは，散布図のように1行を一つの観測値として値を用いる場合や，分類やクラスタリングを行う場合には有用である。これに対し，ロングフォーマットは，属性ごとに積み上げ棒グラフを描いたり，属性ごとに折れ線グラフを描いたりする場合に有用である。  \n",
    "　また，ワイドフォーマットでは，属性を追加した場合にそのデータを参照するシステムの修正が伴ってしまい，変更が容易ではない場合がある。更に，属性の数が多くても，全てのレコードが全ての属性についてのデータがない場合もある。このときワイドフォーマットでは，列（属性）が膨大になると表が疎（空白の項目）を多く持つ状態になってしまう。しかし，ロングフォーマットでは１行で属性とデータを組として保持するため，データが存在しない行は不要であることから，メモリを節約することにつながる。  \n",
    "　表計算ソフトでは，ピボットテーブルを作成することにより，ロングフォーマットからワイドフォーマットに変換できる。ここでは，プログラムを用いて変換する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted = df.melt( ['都道府県'] ,var_name='属性', value_name='値')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>都道府県</th>\n",
       "      <th>属性</th>\n",
       "      <th>値</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>北海道</td>\n",
       "      <td>総人口</td>\n",
       "      <td>5381733.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>青森県</td>\n",
       "      <td>総人口</td>\n",
       "      <td>1308265.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>岩手県</td>\n",
       "      <td>総人口</td>\n",
       "      <td>1279594.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>宮城県</td>\n",
       "      <td>総人口</td>\n",
       "      <td>2333899.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>秋田県</td>\n",
       "      <td>総人口</td>\n",
       "      <td>1023119.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  都道府県   属性          値\n",
       "0  北海道  総人口  5381733.0\n",
       "1  青森県  総人口  1308265.0\n",
       "2  岩手県  総人口  1279594.0\n",
       "3  宮城県  総人口  2333899.0\n",
       "4  秋田県  総人口  1023119.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "melted2 = df.melt( ['都道府県'], ['総人口', '増減', '増減率'],\n",
    " var_name='属性', value_name='値' )\n",
    "melted2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>属性</th>\n",
       "      <th>都道府県</th>\n",
       "      <th>出生数</th>\n",
       "      <th>増減</th>\n",
       "      <th>増減率</th>\n",
       "      <th>死亡数</th>\n",
       "      <th>総人口</th>\n",
       "      <th>転入者数</th>\n",
       "      <th>転出者数</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>三重県</td>\n",
       "      <td>13950.0</td>\n",
       "      <td>-10765.0</td>\n",
       "      <td>-0.592830</td>\n",
       "      <td>20139.0</td>\n",
       "      <td>1815865.0</td>\n",
       "      <td>30612.0</td>\n",
       "      <td>35188.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>京都府</td>\n",
       "      <td>19662.0</td>\n",
       "      <td>-6471.0</td>\n",
       "      <td>-0.247898</td>\n",
       "      <td>25495.0</td>\n",
       "      <td>2610353.0</td>\n",
       "      <td>58586.0</td>\n",
       "      <td>59224.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>佐賀県</td>\n",
       "      <td>7064.0</td>\n",
       "      <td>-5360.0</td>\n",
       "      <td>-0.643587</td>\n",
       "      <td>9702.0</td>\n",
       "      <td>832832.0</td>\n",
       "      <td>15900.0</td>\n",
       "      <td>18622.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>兵庫県</td>\n",
       "      <td>44015.0</td>\n",
       "      <td>-18742.0</td>\n",
       "      <td>-0.338621</td>\n",
       "      <td>55391.0</td>\n",
       "      <td>5534800.0</td>\n",
       "      <td>93099.0</td>\n",
       "      <td>100465.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>北海道</td>\n",
       "      <td>36695.0</td>\n",
       "      <td>-32388.0</td>\n",
       "      <td>-0.601814</td>\n",
       "      <td>60667.0</td>\n",
       "      <td>5381733.0</td>\n",
       "      <td>49407.0</td>\n",
       "      <td>57823.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "属性 都道府県      出生数       増減       増減率      死亡数        総人口     転入者数      転出者数\n",
       "0   三重県  13950.0 -10765.0 -0.592830  20139.0  1815865.0  30612.0   35188.0\n",
       "1   京都府  19662.0  -6471.0 -0.247898  25495.0  2610353.0  58586.0   59224.0\n",
       "2   佐賀県   7064.0  -5360.0 -0.643587   9702.0   832832.0  15900.0   18622.0\n",
       "3   兵庫県  44015.0 -18742.0 -0.338621  55391.0  5534800.0  93099.0  100465.0\n",
       "4   北海道  36695.0 -32388.0 -0.601814  60667.0  5381733.0  49407.0   57823.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = melted.pivot_table( values='値', index='都道府県', columns='属性' )\n",
    "table.reset_index(inplace=True)\n",
    "table.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>属性</th>\n",
       "      <th>出生数</th>\n",
       "      <th>増減</th>\n",
       "      <th>増減率</th>\n",
       "      <th>死亡数</th>\n",
       "      <th>総人口</th>\n",
       "      <th>転入者数</th>\n",
       "      <th>転出者数</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>都道府県</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>三重県</td>\n",
       "      <td>13950.0</td>\n",
       "      <td>-10765.0</td>\n",
       "      <td>-0.592830</td>\n",
       "      <td>20139.0</td>\n",
       "      <td>1815865.0</td>\n",
       "      <td>30612.0</td>\n",
       "      <td>35188.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>京都府</td>\n",
       "      <td>19662.0</td>\n",
       "      <td>-6471.0</td>\n",
       "      <td>-0.247898</td>\n",
       "      <td>25495.0</td>\n",
       "      <td>2610353.0</td>\n",
       "      <td>58586.0</td>\n",
       "      <td>59224.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>佐賀県</td>\n",
       "      <td>7064.0</td>\n",
       "      <td>-5360.0</td>\n",
       "      <td>-0.643587</td>\n",
       "      <td>9702.0</td>\n",
       "      <td>832832.0</td>\n",
       "      <td>15900.0</td>\n",
       "      <td>18622.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>兵庫県</td>\n",
       "      <td>44015.0</td>\n",
       "      <td>-18742.0</td>\n",
       "      <td>-0.338621</td>\n",
       "      <td>55391.0</td>\n",
       "      <td>5534800.0</td>\n",
       "      <td>93099.0</td>\n",
       "      <td>100465.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>北海道</td>\n",
       "      <td>36695.0</td>\n",
       "      <td>-32388.0</td>\n",
       "      <td>-0.601814</td>\n",
       "      <td>60667.0</td>\n",
       "      <td>5381733.0</td>\n",
       "      <td>49407.0</td>\n",
       "      <td>57823.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "属性        出生数       増減       増減率      死亡数        総人口     転入者数      転出者数\n",
       "都道府県                                                                   \n",
       "三重県   13950.0 -10765.0 -0.592830  20139.0  1815865.0  30612.0   35188.0\n",
       "京都府   19662.0  -6471.0 -0.247898  25495.0  2610353.0  58586.0   59224.0\n",
       "佐賀県    7064.0  -5360.0 -0.643587   9702.0   832832.0  15900.0   18622.0\n",
       "兵庫県   44015.0 -18742.0 -0.338621  55391.0  5534800.0  93099.0  100465.0\n",
       "北海道   36695.0 -32388.0 -0.601814  60667.0  5381733.0  49407.0   57823.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "table2 = melted.pivot_table( values ='値', index='都道府県',\n",
    " columns='属性', aggfunc=np.sum )\n",
    "table2.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 欠損値と異常値の取扱い\n",
    "　欠損値は，計測機器の故障などにより値が記録されなかったり，アンケートでの無回答などの理由で値が得られなかったりして，値が欠落していることを示すものである。pandasではNaN，RではNAとして示される。演習6で扱うCSVファイルではセルが空欄になっており，これをpandasで読み込んだときの値はNaNになっている 図表10。記録や回答の際に生じる以外に，ロングフォーマットのデータをワイドフォーマットに変換したときに，完全に表が埋まらないことにより欠損値となる場合がある。\n",
    " \n",
    "　欠損値を無視するのがよいのか，除外するのがよいのか，それらしい値を用いるのがよいのかを検討する必要がある。この判断によってバイアスが生じることもあるため，慎重に扱う必要がある。\n",
    " \n",
    "　ここでは2019年12月の東京都のデータを用いる。解凍ファイルには多くのファイルが含まれるがそのうちの一つをCSV UTF-8形式で保存し直す。  \n",
    "　次にpandasでデータを読み込み，欠損値がどの程度含まれているかを次のプログラムで確認する（実行結果：図表11）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "測定局コード            0\n",
       "日付                0\n",
       "時                 0\n",
       "SO2(ppm)         13\n",
       "NO(ppm)          13\n",
       "NO2(ppm)         13\n",
       "NOx(ppm)         13\n",
       "CO(ppm)         744\n",
       "Ox(ppm)          10\n",
       "NMHC(ppmC)      744\n",
       "CH4(ppmC)       744\n",
       "THC(ppmC)       744\n",
       "SPM(mg/m3)        6\n",
       "PM2.5(ug/m3)      3\n",
       "SP(mg/m3)       744\n",
       "WD(16Dir)         1\n",
       "WS(m/s)           1\n",
       "TEMP(℃)           1\n",
       "HUM(％)            1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# 参考\n",
    "# 「そらまめ君」http://soramame.taiki.go.jp/DownLoad.php\n",
    "# 2019年12月データが手に入らなかったので2020年12月データ\n",
    "df = pd.read_csv(\"202012_13_13101010.csv\", encoding=\"shift_jis\")\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "　次に行う欠損値の処理には次のようなものがある。それぞれのデータフィールドを表示して，例えばNOxの濃度の推移を調べたいとき，NOxの濃度の平均値を調べたいとき，それぞれの場合に適切と考えられる処理について考えてみよう。\n",
    " \n",
    "- 全ての列にデータがあるものを使う：df1 = df.dropna()\n",
    "- 必要な列を選び欠損値がある行を除く：df2 = df[['日付','時','NOx(ppm)']].dropna()\n",
    "- 欠損値を0として扱う：df3 = df.fillna(0)\n",
    "- 前の値で埋める：df4 = df.fillna(method='ffill')\n",
    "- 平均値で埋める：df5 = df.fillna(df.mean())\n",
    " \n",
    "　データの中には，多くのデータからかけ離れた値である外れ値がある。外れ値の中でも原因を特定できるものを異常値という。外れ値は，四分位範囲などの統計量を用いたり，データ間の距離を用いたり，学習16のクラスタリングを用いたりして検出することができる。外れ値が有益なデータの可能性もあるため，その値がどのような原因や理由によって得られたかを考察することが必要である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【参考文献・参考サイト】\n",
    "- 「アメリカ地質調査所 API Documentation」 https://earthquake.usgs.gov/fdsnws/event/1/\n",
    "- 「政府統計の総合窓口（e-Stat）」 https://www.e-stat.go.jp/\n",
    "- 「そらまめ君　環境省大気汚染物質広域監視システム」 http://soramame.taiki.go.jp/DownLoad.php\n",
    "- 「東京大学のデータサイエンティスト育成講座」 塚本邦尊，山田典一，大澤文孝 著　中山浩太郎 監修　マイナビ出版\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 重回帰分析とモデルの決定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題発見とデータに基づく問題解決：予測と制御\n",
    "　問題解決をデータに基づいて行う場合，まず解くべき課題の発見，把握が必要である。課題発見力とは，例えば，海の環境やごみ置き場の清掃状況，地域の商店街の様子や所属するスポーツ部の成績や試験結果など，身の回りの現象に対して，現実があるべき姿（理想）の状態ではないことを知覚し，現実と理想のそれぞれの状態の明確化とそのギャップが解くべき課題であることを具体的に示す力をいう。  \n",
    "　次に，その課題をデータと分析で解決する科学的な問題解決のフレームに落とし込むためには，理想や現実の状態を示す客観的なデータ指標 Y（目的変数，ターゲット変数，教師変数，予測変数，被説明変数）を具体的に定め，その値の変化や変動に何が影響するのかをいわゆる，5W1Hやその発生に至るプロセス要因を洗い出し，論理的な構造モデルをブレーンストーミングなどで作成する必要がある。この際，ブレーンストーミングに使用する論理図には，特性要因図（フィッシュボーンダイアグラム）図表１ ，要因連関図，イシューツリー，ロジックツリー，ロジックモデル等がある。いずれにしても，何が何に影響を与えるかの構図（仮説）を明示することが肝要である。  \n",
    "　データに基づく問題解決のフレームでは，問題を規定する目的変数に対して，その値の変動に影響を与えると思われる要因系の変数（ 図表１ の矢印線の元にある要因）も具体的なデータ指標として記録される。これら要因系のデータ指標群を目的変数Yに対して，説明変数（予測子，要因変数）という。科学的問題解決における予測の問題では，一つもしくは複数（p個）の説明変数の値を使って，目的変数Yの値を規定する構造モデル（回帰モデル）をデータから推測（学習）し，その構造モデルを使って説明変数群の状況に応じたYの値を予測（推測）する。また，各説明変数の値の変化が目的変数にどのように影響するのかという効果を推測（要因分析）したり，シミュレーションを行って最適なYの値を探索したりと，単純にYの値を予測するだけではなく制御する方策に関しても考察を深めることもできる。  \n",
    "　回帰モデルを推測するためのデータは，説明変数や目的変数が個々の対象に対して，観測・記録された構造化データである。例えば，選手の勝率Yを上げることが目的であれば，選手が対象となり，選手のプロファイルデータを収集・整理するが，チームの勝率Yを上げることが目的であれば，チームが対象となり，チームのプロファイルデータを収集・整理することになる。「情報Ⅰ」教員研修用教材の学習22では，中古住宅を対象としたプロファイルデータや生徒を対象としたプロファイルデータを示している 図表２ 。このようなデータの一般形を構造化データ（行列データ）という 図表３ 。  \n",
    "　近年は，スポーツの成績評価や不動産の取引（成約）価格に，予測モデルを使用したスポーツデータサイエンスや不動産データサイエンスなど，データが活用されるデータサイエンス領域が次々と生まれている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 予測のための重回帰モデルと重回帰分析\n",
    "　目的変数Yを p 個の説明変数$X_1,...,X_p$で説明する最も基本的な数学モデルが重回帰モデルである 図表４ 。\n",
    "　重回帰モデルでは，説明変数$X_1,...,X_p$に対して第$i$番目の対象のデータの値$ x_{1i},...,x_{pi} $が与えられたときにその重み付きの合計を求め，それを$y_i$の予測値\n",
    "$\\hat{y}_i$（$y_i$ ハット）とする。\n",
    "\n",
    "$$ \\hat{y}_i = a + b_1x_{1i} + ... + b_px_{pi} $$\n",
    "\n",
    "　このとき，$a$は定数項で,$b_1,b_2,...,b_p$それぞれの説明変数に係る重みで回帰係数という。また，これらを回帰モデルのパラメータという。  \n",
    "　特に，$p=1$，すなわち，説明変数を一つしか使用しないモデルを単回帰モデルといい，これは，直線の式となる。「情報Ⅰ」教員研修用教材の187ページの演習2では，高校生の体力測定データを使って，「50m走のタイム（Y）」の予測式を「立ち幅跳び（X）」を説明変数として，表計算ソフトの「散布図」上で求めている 図表５ 。予測式は以下となる。\n",
    " \n",
    "$$ 50m走(秒)の予測値 = -0.015(秒/cm) × 立ち幅跳び(cm) + 10.731(秒) $$\n",
    "\n",
    "　回帰係数は，「立ち幅跳び」の単位cmを「50m走」の記録の単位である秒に変換する役割を担っている。この場合は，「立ち幅跳び」が1cm長いと「50m走」の予測値が0.015秒ずつ短縮されることを意味する。  \n",
    "　当然，一つの説明変数 X で目的変数 Y の変動が全て説明できるわけではない。そこで，より説明力（予測力）を上げるために，複数の説明変数を使用する必要性が出てくる。それが，重回帰モデルである。同じデータに，説明変数を追加して重回帰モデルを当てはめた結果は，以下となる。\n",
    " \n",
    "50m 走（秒）の予測値 ＝ － 0.012（秒／ cm） × 立ち幅跳び（cm）\n",
    "－ 0.014（秒／ m） × ハンドボール投げ（m）\n",
    "－ 0.040（秒／ kg） × 握力得点（kg）\n",
    "－ 0.025（秒／回） × 上体起こし（回）\n",
    "＋ 10.819（秒）\n",
    "\n",
    "重回帰モデルの回帰係数は，$Y$と$X_1,X_2,...,X_p$に関するデータが与えられたとき，実際の$Y$の観測値と重回帰モデルによる予測値の差（残差），$e_i=y_i-\\hat{y}_i$の2乗和（残差平方和）$ SS = \\sum_{i=1}^{n}{(y_i-\\hat{y}_i)}^2  $を最小にするように求められる。これを最小二乗法という。  \n",
    "　最小二乗法によって求められた予測式に，$X_1,X_2,...,X_p$ の各変数の平均値を代入すると，$Y$の平均値となる。このときの残差は0である。また，この予測値と実際の観測値との相関係数を重相関係数という。重相関係数は，0から1の間の値をとる。単回帰分析の場合，重相関係数はXとYの相関係数ｒの絶対値と等しくなる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重回帰モデルの適合度\n",
    "　最小二乗法により当てはめられた回帰直線が実測されたデータにどの程度，適合しているのかについて，単回帰モデルの場合は，散布図上でデータ点がどの程度直線の近くに集中しているのかで視覚的に判断できる。適合とは，残差が小さいことを意味する。データ全体で，適合度に関する指標として以下がある。\n",
    " \n",
    "- 重相関係数 $R$：  \n",
    "\n",
    "回帰モデルによる期待値$ \\hat{y}i $と実測値との相関係数で，次に示す寄与率の正の平方根として求められる。\n",
    "\n",
    "\n",
    "- 寄与率（決定係数）$R^2$：  \n",
    "\n",
    "目的変数Yの変動の何パーセントが与えられた回帰モデルで説明できたかを示す指標で，Yの平均値まわりの変動（全変動$S_T$）に占めるYの予測値の平均値まわりの変動（回帰による変動$S_R$）の割合として以下の式で求められる（$ 0≦R_2≦1 $）。寄与率は，100%に近いほどモデルがデータに適合していることになる。  \n",
    "\n",
    "$R^2=\\frac{S_R}{S_T}=1-\\frac{S_E}{S_T}$, ここで，$S_T$，$S_R$，$S_E$ は以下である。\n",
    "\n",
    "全変動（全平方和）：$S_T=\\sum_{i=1}^{n}{(y_i-\\bar{y})}^2$\n",
    "\n",
    "\n",
    "回帰による変動（回帰による平方和）：$S_R=\\sum_{i=1}^{n}{(\\hat{y}_i-\\bar{y})}^2$\n",
    "\n",
    "\n",
    "残差変動（残差平方和）：$S_E=\\sum_{i=1}^{n}{(y_i-\\hat{y}_i)}^2$\n",
    "\n",
    "\n",
    "全変動$S_T$がもともとの目的変数の変動を表し，残差変動$S_E$が回帰モデルで説明できない変動を表している。$S_T＝S_R＋S_E$が成立することから，回帰による変動SRは，回帰モデルで説明できたYの変動と考えることができる。\n",
    "\n",
    "\n",
    "- 自由度：  \n",
    "\n",
    "　$S_T$，$S_R$，$S_E$ の各平方和には，それぞれ対応する自由度（独立する成分の数）$f_T$，$f_R$，$f_E$がある。自由度は，その統計量を構成する本質的な（独立した）要素の数で，データの数nと関連する大事な数量である。もともと対象にしているのは，平均値まわりのYの変動 $S_T$ で，自由度はデータ数から制約式の数（平均値の式）1 を引き，$f_T＝n－1$ となる。Yの予測値の変動（回帰による変動SR）の自由度は，回帰パラメータ数と平均制約から，$f_R＝（p＋1）－1$より，$f_R＝p$となり，その予測値とYの変動（残差変動$S_E$）の自由度は，$f_E＝n－1－p$ となる。全変動の分解 $S_T＝S_R＋S_E$ と同様，自由度の分解 $f_T＝f_R＋f_E$ も成立する。\n",
    " \n",
    " \n",
    "- 標準誤差：\n",
    "\n",
    "残差の自由度を調整した標準偏差である。（$S_E /f_E$）の平方根で求められ，Ｙと同じ具体的な測定単位を持つ。例えば50ｍ走の記録タイムの予測であれば，単位は秒である。この値が小さいほど良いモデルとなるが，単純に，残差変動$S_E$が小さくなる（寄与率$R^2$が上がる）だけでは達成されない。残差変動の自由度  $f_E$が大きいことも重要となる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重回帰分析のコンピュータでの実行と出力\n",
    "1. 表計算ソフトでの重回帰分析の実行\n",
    "\n",
    "　重回帰分析は，Excelの「データ」メニューから「データの分析」（アドインで設定）を選択することで簡単に実行できる 図表６, ７, ８ 。\n",
    "\n",
    "ここでは，重回帰分析の機能を追加したExcelで体力測定のデータ（高校1年生男子）で重回帰分析を行い，その出力を示す 図表９ 。\n",
    "\n",
    "　なお，ここで示した図表は表計算ソフトの一例としてExcelで示したが，Excelの出力結果は必要に応じて，レポート等では書き直す必要がある。\n",
    " \n",
    "- 「重相関$R$」→「重相関係数$R$」\n",
    "- 「重決定$R^2$」→「寄与率（決定係数）$R^2$」\n",
    "- 「補正$R^2$」→「自由度修正済み$R^2$」\n",
    "\n",
    "　この出力から，予測値と実測値の相関係数（重相関係数）が0.725であること，モデルの適合度を示す寄与率$R^2$が52.5%であることが分かる。「立ち幅跳び」のみを説明変数とした単回帰モデルの寄与率が45%だったので 図表５ ，説明変数を増やしたことで，モデルの適合度（Yの変動の説明力）が上がったことが分かる。\n",
    " \n",
    "　回帰係数の推定値は，「係数」の列の箇所に出力される。また，分散分析表として，先に示した変動和の分解と自由度の分解が出力される 図表10。ここで，回帰の自由度はモデルの項の数（説明変数の数）を表し，モデルの複雑度に相当するが，モデルの複雑度を上げれば，残差の自由度が少なくなることも留意する必要がある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 統計ソフト R での重回帰分析の実行\n",
    "\n",
    "　重回帰分析をRで実行する場合は，以下のコードとなる。ここでは，前項のExcelで使用した高校1年生男子の体力測定のデータを用いる。Rでデータを読み込む場合は，Excel形式をCSV形式（変数名の制限よりX50m走としてある）にしてから読み込む（2行目）。4行目が実際に推測したい回帰モデル式を入力する部分である。5行目は，分析結果を要約するコードである。これは忘れないようにする必要がある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル選択（モデリング）\n",
    "　複数の説明変数の中には，目的変数の予測に役立たないものが含まれている可能性がある。そこで，モデル式の構築にあたっては，説明変数の取捨選択が重要な課題となる。これを変数選択もしくはモデル選択（モデリング）という。その際，寄与率$R^2$ができるだけ大きくなることが望ましいが，説明変数を増やせば増やすほど単純に大きくなる。また，モデルに説明変数の二次項や三次項（曲線の当てはめ，非線形モデル）を追加することも説明変数を増やしたことと同じ効果となり，複雑なモデルほど寄与率$R^2$は大きくなる。しかし，欠点（過剰適合，過学習）も生じる。  \n",
    "　例えば，データが2人分しかない体力測定のデータで散布図を作成し直線を当てはめた場合，必ず二つのデータ点が直線上に乗り，寄与率$R^2$は100%になる。3人分であれば，二次項を含め多項式でモデル化するとやはり寄与率$R^2$は100%となる 図表11。しかし，これらのモデルでは，残差の自由度は0となり，標準誤差は無限大となり計算できない。つまり，新しいデータに対して予測力がまるでないことになる。したがって，ある程度の残差の自由度を残しつつ，寄与率$R^2$を上げることが望ましい。  \n",
    "　そのため，重回帰分析では，残差の自由度を考慮した，下記のようなモデルの良し悪しを測る指標を参考にしながら，説明変数の選択（モデル選択）を注意深く行う作業が必要となる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 自由度調整済み寄与率$R^2$\n",
    "\n",
    "$$1- \\frac{S_E/(n-p-1)}{S_T/(n-1)}$$\n",
    "\n",
    "自由度調整済みの寄与率$R^2$が大きなモデルほど説明力のある良いモデルということになる。これ以外にも，モデル選択の基準として，AIC（赤池の情報量規準）などの指標がある。AICは小さいほど良いモデルとされる。\n",
    "\n",
    "\n",
    "- 回帰係数の有意差の検定\n",
    "\n",
    "　体力測定の重回帰分析の出力の箇所には，各回帰係数が母集団上で0であるか否かの統計的仮説検定（帰無仮説  $H_0：b_j = 0$）の検定結果を示す有意確率（P-値）の列がある 図表９ 。この値が，1%もしくは5%以下であれば帰無仮説は棄却され，その説明変数の値の変化が統計的に有意にYの値の変化に影響を与えるという対立仮説が採択されたことになる。この例では，「立ち幅跳び」は1%有意，「ハンドボール投げ」は5%有意であるが，「握力」や「上体起こし」に有意差はない。つまり，有意差の出ない変数は「50m走」の値の変化に影響を与えていない可能性が高いと判断される。そこで，この二つの説明変数を外してモデルを作成し直してみる，更に新しい別の説明変数を加えて，同様な分析を繰り返すなどを行い，最適なモデルを探索する。\n",
    " \n",
    "\n",
    "- 統計ソフトによる自動変数選択\n",
    "\n",
    "　Rなどの統計ソフトには，統計的な基準で変数選択を自動で行う機能がある。変数選択には，総当法（全ての変数の組み合わせを尽くす方法），変数増加法（一つの変数からだんだんと変数を増やしていく方法），変数減少法（全ての変数を採用したモデルから変数を減らしていく方法），変数増減法（ステップワイズ法：変数を減らしたり増やしたりする方法）がある。変数を選択する基準が一つではなく，また，説明変数間の相関の強弱によって，どの方法を選ぶかで選択される変数，すなわち最終的に採択されるモデルの結果は異なる。何が変数として選択されたかを全くブラックボックスとし単純に予測モデルを構築したい場合は，自動変数選択の機能は便利ではあるが， 説明変数が目的変数に与える効果に言及し要因分析を行う場合は，自動変数選択は安易に使用すべき手法ではない。参考までに，Rのコード（変数増減法）は以下となる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# データの読み込み\n",
    "high_male <- read.csv(\"high_male_data.csv\")\n",
    "# 全ての説明変数によるステップワイズ法（変数増減法）による重回帰分析\n",
    "res <- lm(X50 ｍ走 ~ 立ち幅跳び + ハンドボール投げ + 握力得点 + 上体起こし得点 , data = high_male)\n",
    "step(res)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "ダミー変数  \n",
    "重回帰分析では，目的変数も説明変数も基本的には量的な変数である必要があるが，説明変数に質的な変数を用いることがある。この場合は，説明変数の質的な属性の有無を，0 と 1 の数値で対応させた変数（ダミー変数と呼ぶ）で表現し直し，ダミー変数を説明変数として重回帰分析を行う。目的変数が質的変数の場合は，ロジスティック回帰分析を行う。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【参考文献・参考サイト】\n",
    "- 「多変量解析法入門 （ライブラリ新数学大系）」 永田靖，棟近雅彦 著　サイエンス社（2001）\n",
    "- 「図解でわかる回帰分析―複雑な統計データを解き明かす実践的予測の方法」 涌井良幸，涌井貞美 著　日本実業出版社（2002）\n",
    "- 「理科ネットワーク　デジタル教材　『科学の道具箱』」 https://rika-net.com/contents/cp0530/start.html\n",
    "- 「生徒のための統計活用～基礎編～」（生徒用，指導用） 渡辺美智子 他 著　総務省政策統括官（統計基準担当） 編　日本統計協会（2016）\n",
    "- 「高校からの統計・データーサイエンス活用～上級編～」（生徒用，指導用） 渡辺美智子 他 著　総務省政策統括官（統計基準担当） 編　日本統計協会（2017）\n",
    "- 「問題解決力向上のための統計学基礎―Excel によるデータサイエンススキル」 迫田宇広 , 高橋将宜，渡辺美智子 著　日本統計協会（2014）\n",
    "- 「実践ワークショップ Excel 徹底活用 統計データ分析 改訂新版」 渡辺美智子，神田智弘 著　秀和システム（2008）\n",
    "- 「文化情報学事典」 渡辺美智子 他 編　村上征勝 監修　勉誠出版（2019）\n",
    "- 「統計学Ⅲ : 多変量データ解析法オフィシャルスタディノート」 岩崎学，足立浩平，渡辺美智子，宿久洋，芳賀麻誉美 著　日本統計学会・日本行動計量学会 編　日本統計協会（2017）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 主成分分析と次元削減\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 主成分分析とは？ ～新しい指標（特徴量）を効果的に作成する方法～\n",
    "\n",
    "　対象の特徴を一般にデータで表す際に，単一の変数だけではなく多くの変数を使うことで，対象の特徴をより詳細に分析することができる。学習13で挙げた生徒を対象とした体力測定のデータでは，体力に関する複数の種類の測定項目を使って生徒の体力の状況が調査されている。また，中古住宅のデータは，住宅の特徴を床面積や築年数など，やはり複数の変数で住宅の特徴が記述されている。しかし，変数の数が増えれば，データから個々の対象の特徴を総合的に捉えることが難しくなる。主成分分析を使えば，このような多変量（高次元）のデータに対して，変数の間の共分散や\n",
    "相関の強い変数同士をまとめて，個々の対象の違いを最も大きくするような主成分と呼ばれる新しい特徴量（変数）を作成することができる。その主成分（変数）軸を使って，個々の対象のポジショニングの把握や対象全体の分類を効果的に行うことができる。  \n",
    "　具体的には，元の$p$個の変数$（X_1, X_2,･･･,X_p）$から，情報を損失することなく線形結合（重み付きの合計）によって，$p$個の互いに独立な合成変数を主成分$（Z_1, Z_2,･･･,Z_p）$として作成する。  \n",
    "　主成分は分散が最も大きくなるような順番で作成され，下位の主成分になるに従って，対象間で主成分の値（主成分得点）の変動（分散）が小さくなる 図表１ 。そこで，対象の弁別に対して寄与の小さい下位の方の主成分を捨て上位の主成分のみ採用することで，元の個数pより少ない数で，対象の特徴をプロファイルすることが可能になる。このことを次元削減（次元縮約）といい，特に高次元の変数を扱う画像データの処理では，機械学習の一つの手法として使用されている。ま\n",
    "た，主成分分析によって，対象をうまく説明する新しい特徴量を見いだすこともできる。簡単な例で説明する。50人の生徒の成績の状況を表す5科目の試験に関する成績データがあるとする 図表２ 。\n",
    "\n",
    "　5次元のデータを1次元にするために合計得点が計算され，50人の生徒を合計得点で順位付ける，ということはよく行われている。このとき，合計得点は生徒の総合能力を表す一つの特徴量と考えることができる。合計得点は各科目を同じ重み「1」で合計した変数である。\n",
    " \n",
    "$$ 合計得点＝1×国語＋1×英語＋1×数学＋1×物理＋1×化学 $$\n",
    "\n",
    "ここで，各科目の係数を一般化して，a，b，c，d，eとして作成される合成変数を考えてみよう。\n",
    "\n",
    "$$ 合成変数＝a×国語＋b×英語＋c×数学＋d×物理＋e×化学 $$\n",
    "\n",
    "　a，b，c，d，eにいろいろな数値を与えると，単純な合計得点以外の合成変数をいろいろ作成することができる。主成分分析で作成する主成分とは，係数 a，b，c，d，eを50人の生徒の合成変数の値（主成分得点）が最もばらつくように，つまり，分散が最大となるように決定される一つの変数である。ばらつきが大きいことがその変数の情報量の大きさに対応する。つまり，生徒の成績のパターンの違いを最もよく表す特徴量となる。  \n",
    "　最初に求められる主成分を第1主成分という。次に求められる第2主成分は，第1主成分と独立である（無相関）という条件の下で，分散が最大になるように求められる。このように求められる主成分同士は，互いに相関しない，情報が重複しない特徴量となる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 表計算ソフトで主成分を求めてみる\n",
    "\n",
    "　合成変数を作成する場合，二つの立場がある。　\n",
    "\n",
    "1. 5科目の得点のレベル（位置）をそろえる（平均を同じにする，平均からの偏差にする）。\n",
    "2.  5科目の得点のレベル（位置）とばらつきの大きさの双方をそろえる（平均と標準偏差を同じにする，基準化（標準化）する，または，偏差値にする）。  \n",
    "\n",
    "　単位が異なる複数の変数を扱う場合は，②の基準化を行い，単位をそろえる必要がある。単位が同じ場合は，①と②の双方の立場で分析が可能である。①では，各変数の分散の大きさを考慮した係数が求まり，②では，各変数の分散の違いを無視した係数が求まる。  \n",
    "　表計算ソフトを使用して分散を最大化する係数を求めるには，計算結果（主成分分析の場合は，分散）を目標値として設定し，制約条件を指定した上で変数のセル（係数のセル）を変化させ目標値を最大化させる係数を求める最適化機能を使う。ここでは，一例としてExcelで示す。Excelでの最適化機能は「ソルバー」と呼ばれる。「ソルバー」は，「ファイル」メニューの「オプション」から「アドイン」で，「ソルバーアドイン」を設定すれば，「データ」メニューから利用できる\n",
    "図表３（「挿入」メニューの「アドイン」から設定する場合もある）。   \n",
    "　まず，成績データを②の立場で基準化したデータに変換し，基準化したデータを使って第1主成分，第2主成分の係数と各主成分得点を求めるためのシートを事前に準備する 図表４ 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 第 1 主成分を求める手順\n",
    "\n",
    "\n",
    "\n",
    "1. それぞれの主成分の係数のセルを用意し，初期値（ここでは1）をあらかじめ入力しておく。\n",
    "2. 各生徒の，主成分係数によって求められる重み付き合計「主成分得点」の列を作り，SUMPRODUCT関数であらかじめ得点を全て計算し，主成分得点の平均と分散はAVERAGE関数やVARP関数で求めておく。\n",
    "3. 主成分の分散は係数の絶対値を大きくすればするほど大きくなる。そこで，全ての係数の2乗和を1とする制約条件の下で，分散を最大化させる必要がある。制約条件のセルをSUMSQ関数で作成しておく 図表４ 。\n",
    "4. 第1主成分を求める。「データ」メニューから，「ソルバー」を選択し，「ソルバーのパラメータ」ダイアログでパラメータ設定をする 図表５ 。  \n",
    "目的セル：第1主成分得点の分散を計算したセル\n",
    "目標値：最大値  \n",
    "変数セル：第1主成分係数のセル  \n",
    "制約条件：係数の2乗和のセル=1  \n",
    "チェックボックス「非負数」はチェックしない\n",
    "\n",
    "5. 「解決」をクリックする。「ソルバーの結果」ダイアログが表示され，「OK」をクリックすると，主成分係数と得点，最大化された分散などの結果がシートに反映される 図表６ 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 第 1 主成分の解釈\n",
    "\n",
    "　第1主成分の最大化された分散は，3.12である。基準化された得点データの分散はそれぞれ1である。5科目の分散の合計「5」がデータ全体の分散の量と考えると，主成分1のみで，3.12 ／ 5=0.624，すなわち，全体の62.4％の情報が集約されたことになる。これを主成分の寄与率という。また，各科目がどのような重みで計算されたものかを主成分の係数の大きさから解釈すると，第1主成分は，やや理系科目に重きを置いた総合得点と考えることができる。この主成分が分散が大きくなるように数学的に導いた，50人の生徒の弁別性が最も大きくなる特徴量となる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 第 2 主成分を求める手順\n",
    "\n",
    "\n",
    "1. 第1主成分と第2主成分は独立という制約条件がある。そのため，それぞれの主成分得点の相関係数をCORREL関数で求めるセルを作っておく。\n",
    "2. 第2主成分係数に関しても，2乗和を1とする制約を課すため，SUMSQ関数で2乗和を計算するセルを作成する 図表４ 。\n",
    "3. ソルバーのパラメータ設定を行う 図表７ 。  \n",
    "目的セル：第2主成分得点の分散を計算したセル  \n",
    "目標値：最大値  \n",
    "変数セル：第2主成分係数のセル  \n",
    "制約条件の追加：  \n",
    "第2主成分の係数の2乗和のセル=1  \n",
    "第1と第2主成分得点の相関係数=0  \n",
    "チェックボックス「非負数」はチェックしない"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 第 2 主成分の解釈\n",
    "\n",
    "　第2主成分の最大化された分散は，1.16となる図表８ 。主成分2の寄与率は，1.16 ／ 5=0.231，全体の23.1％の情報をもつ特徴量となる。主成分1と主成分2の寄与率の合計（主成分2までの累積寄与率）は，0.624＋0.231＝0.855で，新しく作られた二つの特徴量で，もともとあった五つの変数の情報の85.5%をカバーしたことになる。これは，生徒の5科目の成績のパターンの特徴が二つの変数でだいたい読み取れることを意味しており，これが次元削減の意味である。また，第2主成分は，第2主成分係数から 図表８ ，以下となる。\n",
    " \n",
    "$$第 2 主成分＝（0.68 ×国語の基準化得点＋ 0.50 ×英語の基準化得点 ）－（0.27 ×数学の基準化得点＋ 0.40 ×物理の基準化得点＋ 0.22 ×化学の基準化得点 ）\n",
    "$$\n",
    "\n",
    "　これは，国語や英語に重みを置いた総合得点と，数学や理科に重みを置いた総合得点の差（対比），すなわち，生徒の教科の興味・関心の方向性を識別する変数\n",
    "と考えることができる。  \n",
    "　主成分分析の結果，第1主成分と第2主成分の累積寄与率が80%を超えており，生徒の5科目の成績は，全科目の総合的な得点と生徒の教科の興味・関心の方向性という二つの観点でほぼ特徴付けられることが分かる。また，各主成分の得点を見ることで，その二つの観点に関する生徒の数量的評価もできることになる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 主成分負荷量（主成分と元の変数との相関係数）\n",
    "\n",
    "　主成分の解釈に主成分係数を使う以外に，主成分と元の変数との相関係数（主成分負荷量または因子負荷量）を使うこともある。アドインされた「データ」メニューの「データ分析」にある「相関」で，相関行列が出力される。この場合，主成分負荷量は，相関行列の枠で囲った部分となる。相関係数は「数学Ⅰ」の「データの分析」の単元で学習しているので，生徒には分かりやすい指標である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 相関行列と共分散行列（平均からの偏差データと基準化データ）\n",
    "\n",
    "　統計ソフトウェアの主成分分析の数学的な演算処理は，行列を用いて一度に全ての主成分に関する統計量が計算できるアルゴリズム（行列の固有値分解）を採用している。そのため，①平均からの偏差データに変換したデータに基づく主成分分析は，実際には，元の変数の分散共分散行列を固有値分解して求められている。②基準化データに変換したデータに基づく主成分分析は相関行列の固有値分解である。そのため，Rなど統計ソフトを使う場合は，分析者が，分散共分散行列か相関行列のどちらを対象にした分析を行うのかを指定しなければならないので，その意味を知っておく必要がある。また，統計ソフトウェアから出力される各主成分の分散を意味する統計量は固有値，各主成分の係数は，固有ベクトルという名称で出力されるので，用語と意味の対応付けも知っておく必要がある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分析事例（野球選手の成績評価）\n",
    "　主成分分析の簡単な事例として，2017年の日本プロ野球の規定打席数に達した両リーグの55選手の打撃成績のデータを用いた例を紹介する。「打率」，「得点数」，「安打数」，「二塁打数」，「三塁打数」，「本塁打数」，「打点数」，「盗塁数」の8変数を使い，相関行列に基づく主成分分析を行った。その結果が右の表である図表10 図表11。  \n",
    "　例として，第2主成分までを採択して，結果の解釈を行ってみる。第2主成分までの累積寄与率が約70％強であることから 図表10，選手の元の8指標での打撃評価の変動の約70%が二つの縮約された主成分で説明できることになる。第1主成分の寄与率は約40%で図表10，主成分係数はいずれも同じ＋の符号であることから図表11，打率，安打，得点，二塁打を中心に「総合活躍度」を示した指標と解釈できる。第2主成分は寄与率が約30%で 図表10，主成分係数の符号と絶対値の大きさから図表11，打点と本塁打，三塁打と盗塁の成績の対比を表した指標と考えられる。つまり，第2主成分は打撃のスタイルを意味する指標で，第2主成分得点が＋に高ければ長打力を活かすタイプの選手，−に低ければ走力を活かした選手と評価できる。  \n",
    "　第1主成分得点と第2主成分得点で選手のポジショニングを示した散布図を作成すれば，中心円から離れた選手ほど，二つの指標（主成分）の観点で特異であることが分かる図表12。上位と下位の主成分を組み合わせて散布図に示す場合は，基準化した主成分得点を使用することもある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rで体力測定のデータを分析してみよう\n",
    "　主成分分析のRのコマンドはprcompで，主成分の英語，principal componentに因る。学習13でも紹介した体力測定のデータで，4列目の「握力」から11列目の「ハンドボール投げ」までの8変数を使って主成分分析を行う図表13。Rのコードは下記となる。ここで，共分散行列に基づく分析の場合は，コマンド内のオプションで scale = F を，相関行列のときは scale = T を入力する。体力測定の場合は，変数の単位も異なることから相関行列に基づく分析を行う。出力が英語であることから，赤字でその意味を付している。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# 身長，体重，座高を除き，握力からハンドボール投げまでの 8 変数を使った主成分分析\n",
    "# 相関行列\n",
    "# データセット ( 第 4 列から第 11 列 ) のインポート\n",
    "high_male2 <- read.csv(\"high_male2.csv\")\n",
    "high_male3 <- high_male2[c(4:11)]\n",
    "# 相関行列に基づく主成分分析 prcomp の実行\n",
    "(res2 <- prcomp(high_male3, scale=T))\n",
    "summary(res2)\n",
    "pc <- res2$x\n",
    "# 主成分得点のファイルへの掃き出し\n",
    "write.csv(pc, le = \"pca_cor.csv\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【参考文献・参考サイト】\n",
    "- 「主成分分析 ― 講座 情報をよむ統計学〈8〉」上田 尚一 著　朝倉書店（2003）\n",
    "- 「図解入門よくわかる多変量解析の基本と仕組み」山口 和範 著　秀和システム（2004）\n",
    "- 「理科ネットワーク　デジタル教材　『科学の道具箱』」https://rika-net.com/contents/cp0530/start.html\n",
    "- 「日本野球機構　シーズン成績」（2017）http://npb.jp/bis/2017/stats/\n",
    "- 「高校からの統計・データーサイエンス活用～上級編～」（生徒用，指導用）渡辺美智子 他 著　総務省政策統括官（統計基準担当）編　日本統計協会（2017）\n",
    "- 「問題解決力向上のための統計学基礎―Excel によるデータサイエンススキル」迫田宇広 , 高橋将宜 , 渡辺美智子 著　日本統計協会（2014）\n",
    "- 「実践ワークショップ Excel 徹底活用 統計データ分析 改訂新版」渡辺美智子，神田智弘著　秀和システム（2008）\n",
    "- 「文化情報学事典」渡辺美智子 他 編　村上征勝 監修　勉誠出版（2019）\n",
    "-  「統計学Ⅲ : 多変量データ解析法オフィシャルスタディノート」岩崎学，足立浩平，渡辺美智子，宿久洋，芳賀麻誉美 著　日本統計学会・日本行動計量学会 編　日本統計協会（2017）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
